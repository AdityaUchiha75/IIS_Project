{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import opencv_jupyter_ui as jcv2\n",
    "from feat import Detector\n",
    "from IPython.display import Image\n",
    "from feat.utils import FEAT_EMOTION_COLUMNS\n",
    "import time as t\n",
    "from time import sleep\n",
    "from furhat_remote_api import FurhatRemoteAPI\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import trace\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import threading\n",
    "import os\n",
    "from torch import nn\n",
    "import torch\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Furhat setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FURHAT_IP = \"127.0.1.1\"\n",
    "\n",
    "furhat = FurhatRemoteAPI(FURHAT_IP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Successfully changed my LEDs', 'success': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "furhat.set_led(red=100, green=50, blue=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no']\n"
     ]
    }
   ],
   "source": [
    "furhat.say(text=\"Would you like to take a break?\", blocking=True)\n",
    "result = furhat.listen()\n",
    "texts = result.__dict__['_message'].split(' ')\n",
    "print(texts)\n",
    "if \"yes\" in texts:\n",
    "    furhat.say(text=\"Great! let's take a 10 second break\", blocking=True)\n",
    "    furhat.gesture(name=\"CloseEyes\")\n",
    "    sleep(10)\n",
    "    furhat.gesture(name=\"OpenEyes\")\n",
    "    furhat.say(text=\"Let's continue.\")\n",
    "      # Exit the loop if the user wants a break\n",
    "\n",
    "elif \"no\" in texts:\n",
    "    furhat.say(text=\"Great! Let's continue.\")\n",
    "      # Exit the loop if the user doesn't want a break\n",
    "\n",
    "else:\n",
    "    furhat.say(text=\"I'm sorry, I didn't get that, can you repeat it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, features_in=2, features_out=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(features_in, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, features_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "    \n",
    "path=Path(os.getcwd()).parent\n",
    "DIR_PATH=str(path) + '\\\\'\n",
    "\n",
    "device = \"cpu\" #\"cuda\" if torch.cuda.is_available() else\n",
    "model = torch.load(Path(DIR_PATH + 'models\\\\best_model_12.pt') ).to(device) #best_emo_model(local_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression = {\"anger\": 6, \"disgust\": 5 , \"fear\": 4, \"happiness\": 1, \"neutral\": 0, \"sadness\": 2, \"surprise\": 3}\n",
    "def return_emo(loc):\n",
    "    for key,_ in expression.items():\n",
    "        if expression[key] == loc:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMO_LIST= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ray.remote\n",
    "def capture_and_return_emos(t):\n",
    "    fl = False\n",
    "    start= time.time()\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    print(\"Capture on\")\n",
    "    flag=True\n",
    "    while flag: #or not event.is_set()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to capture the frame.\")\n",
    "            break\n",
    "\n",
    "        detected_faces = detector.detect_faces(frame)\n",
    "        detected_landmarks = detector.detect_landmarks(frame, detected_faces)\n",
    "        detected_aus = detector.detect_aus(frame, detected_landmarks)\n",
    "\n",
    "        for faces,au_units in zip(detected_faces,detected_aus): #access only one frame\n",
    "            for i in range(len(faces)): #access all faces detected in the frame\n",
    "                au_arr=model(torch.tensor(au_units[i]).to(device)).cpu()\n",
    "                max_loc=np.argmax(au_arr.softmax(dim=0).numpy())\n",
    "                emotion=return_emo(max_loc)\n",
    "                EMO_LIST.append(emotion)\n",
    "                x, y, w, h, p = faces[i]\n",
    "                # Drawing a rectangle around the detected face\n",
    "                cv2.rectangle(frame, (int(x), int(y)), (int(w), int(h)), (0,0 , 255), 2)\n",
    "\n",
    "                # Displaying the emotion label on top of the rectangle\n",
    "                cv2.putText(frame, emotion, (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "                stop = time.time()\n",
    "                # if event.is_set():\n",
    "                #     flag=False\n",
    "                #     break\n",
    "                if stop - start >= t:\n",
    "                    print(fl)\n",
    "                    fl = True\n",
    "                    flag=False\n",
    "                    break                                   \n",
    "                 #key == 27:\n",
    "\n",
    "    # jcv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    print(\"Capture off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event = threading.Event()\n",
    "# stop_threads = False\n",
    "# t1 = thread_with_trace(target = capture_and_return_emos(event,)) #lambda : stop_threads,\n",
    "\n",
    "# t1.start()\n",
    "# time.sleep(5)\n",
    "# event.set()\n",
    "# t1.kill()\n",
    "# # start = time.time()\n",
    "# # stop = time.time()\n",
    "# t1.join()\n",
    "# # if stop - start >=7:\n",
    "#     # stop_threads = True\n",
    "# fl=False\n",
    "\n",
    "# process1 = multiprocessing.Process(target=capture_and_return_emos(lambda: fl,))\n",
    "# process1.start()\n",
    "# start= time.time()\n",
    "# time.sleep(7)\n",
    "# stop = time.time()\n",
    "# if stop - start >= 7:\n",
    "#     fl = True\n",
    "# else:\n",
    "#     fl= False\n",
    "    \n",
    "# process1.terminate()\n",
    "# process1.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Audit\\anaconda3\\envs\\iis\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\Audit\\anaconda3\\envs\\iis\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\Audit\\anaconda3\\envs\\iis\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Capture off\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "# @ray.remote\n",
    "def bsay(line):\n",
    "    furhat.say(text=line, blocking=True)\n",
    "    sleep(3)\n",
    "    furhat.say(text=line, blocking=True)\n",
    "    sleep(3)\n",
    "\n",
    "\n",
    "\n",
    "# # Execute func1 and func2 in parallel.\n",
    "# ray.get([bsay.remote(\"Hello everybody!!\"), capture_and_return_emos.remote(6)])\n",
    "queue = multiprocessing.Queue()\n",
    "# process1 = multiprocessing.Process(target=bsay, args=(\"Hello everybody!!\",))\n",
    "process2 = multiprocessing.Process(target=capture_and_return_emos(6)) #, daemon=False\n",
    "# process1.start()\n",
    "process2.start()\n",
    "bsay(\"Hello everybody!!\")\n",
    "\n",
    "# process1.join()\n",
    "\n",
    "# process1.terminate()\n",
    "# process2.terminate()\n",
    "\n",
    "\n",
    "\n",
    "# p1 = threading.Thread(target=capture_and_return_emos(6))\n",
    "# p1.start()\n",
    "# p2 = threading.Thread(target=bsay(\"Hello everybody!!\"))\n",
    "# p2.start()\n",
    "# p2.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "process1.start()\n",
    "process2.start()\n",
    "\n",
    "\n",
    "process1.join()\n",
    "process2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral 4\n",
      "neutral 4\n"
     ]
    }
   ],
   "source": [
    "#len(EMO_LIST)\n",
    "max=0\n",
    "for i in np.unique(EMO_LIST):\n",
    "    print(i, np.char.count(EMO_LIST, i).sum())\n",
    "    if np.char.count(EMO_LIST, i).sum() > max:\n",
    "        max = np.char.count(EMO_LIST, i).sum()\n",
    "        emo = i\n",
    "print(emo, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'anger',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'anger',\n",
       " 'happiness']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMO_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.02\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    print('', end='')\n",
    "    ft=time.time()\n",
    "\n",
    "print('\\n', f'{(ft-st):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1():\n",
    "    print(\"Task 1 assigned to thread: {}\".format(threading.current_thread().name))\n",
    "    print(\"ID of process running task 1: {}\".format(os.getpid()))\n",
    " \n",
    "def task2():\n",
    "    print(\"Task 2 assigned to thread: {}\".format(threading.current_thread().name))\n",
    "    print(\"ID of process running task 2: {}\".format(os.getpid()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ID of process running main program: {}\".format(os.getpid()))\n",
    "\n",
    "print(\"Main thread name: {}\".format(threading.current_thread().name))\n",
    "\n",
    "t1 = threading.Thread(target=task1, name='t1')\n",
    "t2 = threading.Thread(target=task2, name='t2')\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "t1.join()\n",
    "t2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
